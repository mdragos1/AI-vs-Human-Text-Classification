# 🧠 AI vs Human Text Classification

This project tackles a **binary classification** task: detecting whether a text was written by a human or generated by an AI. It compares the performance of three deep learning models: **CNN1D**, **BERT**, and an **Autoencoder-based classifier**, trained on a cleaned and balanced version of the AI vs Human Text dataset.

---

## 📊 Dataset

- **Total Samples**: ~487,000
- **Class Distribution**:  
  - AI-generated: 1/3  
  - Human-written: 2/3
- **Sampling Strategy**: To combat class imbalance, we sample 50% from each class
- **Preprocessing**:
  - Extract only the **first sentence** of each text
  - Remove **non-alphanumeric characters**, **extra spaces**, and **stopwords**
  - **Tokenization**, **padding**, and **vocabulary building**

---

## 🧪 Sampling Per Model

| Model        | Sample Count |
|--------------|--------------|
| CNN1D        | 300,000      |
| BERT         | 25,000       |
| Autoencoder  | 5,000        |

---

## 🏗️ Model Architectures

### 🌀 CNN1D

- **Structure**: Embedding → Conv1D → MaxPooling → Dropout → Dense
- **Grid Search Parameters**:
  - `embedding_size`: [32, 64, 128]
  - `filters`: [32, 64, 128]
  - `dropout_rate`: [0.3, 0.5, 0.7]
- ✅ **Best Accuracy**:
  - **Validation**: 98.01%
  - **Test**: 97.89%

---

### 🔠 BERT-like Transformer

- Custom transformer model (no positional embeddings)
- **Structure**: Embedding → 2× Transformer Blocks → Global Avg Pooling → Dropout → Dense
- **Grid Search Parameters**:
  - `num_heads`: [2, 4]
  - `num_blocks`: [2, 4]
  - `feedforward_dim`: [128, 256]
- ✅ **Best Accuracy**:
  - **Validation**: 88.9%
  - **Test**: 88.26%

---

### 🧬 Autoencoder + Classifier

- Encoder: Embedding → LSTM → BatchNorm → Repeat Vector → LSTM → Dense
- Classifier: Dense → Dropout → Dense → Sigmoid
- **Grid Search Parameters**:
  - `embedding_size`: [256, 512]
  - `lstm_size`: [128, 256]
  - `dropout_rate`: [0.3, 0.5]
- ⚠️ **Best Accuracy**:
  - **Validation**: 56.13%
  - **Test**: 53.2%

---

## 📈 Results Summary

| Model        | Test Accuracy |
|--------------|---------------|
| 🌀 CNN1D      | 🟢 97.89%      |
| 🔠 BERT       | 🟡 88.26%      |
| 🧬 Autoencoder| 🔴 53.2%       |

---

## 🧠 Observations

- **CNN1D** outperforms the rest — fast, accurate, and robust
- **BERT** performs well but shows slight **overfitting** past epoch 7
- **Autoencoder** struggles due to low latent diversity and complexity mismatch

---


