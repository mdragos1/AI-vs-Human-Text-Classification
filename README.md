# ğŸ§  AI vs Human Text Classification

This project tackles a **binary classification** task: detecting whether a text was written by a human or generated by an AI. It compares the performance of three deep learning models: **CNN1D**, **BERT**, and an **Autoencoder-based classifier**, trained on a cleaned and balanced version of the AI vs Human Text dataset.

---

## ğŸ“Š Dataset

- **Total Samples**: ~487,000
- **Class Distribution**:  
  - AI-generated: 1/3  
  - Human-written: 2/3
- **Sampling Strategy**: To combat class imbalance, we sample 50% from each class
- **Preprocessing**:
  - Extract only the **first sentence** of each text
  - Remove **non-alphanumeric characters**, **extra spaces**, and **stopwords**
  - **Tokenization**, **padding**, and **vocabulary building**

---

## ğŸ§ª Sampling Per Model

| Model        | Sample Count |
|--------------|--------------|
| CNN1D        | 300,000      |
| BERT         | 25,000       |
| Autoencoder  | 5,000        |

---

## ğŸ—ï¸ Model Architectures

### ğŸŒ€ CNN1D

- **Structure**: Embedding â†’ Conv1D â†’ MaxPooling â†’ Dropout â†’ Dense
- **Grid Search Parameters**:
  - `embedding_size`: [32, 64, 128]
  - `filters`: [32, 64, 128]
  - `dropout_rate`: [0.3, 0.5, 0.7]
- âœ… **Best Accuracy**:
  - **Validation**: 98.01%
  - **Test**: 97.89%

---

### ğŸ”  BERT-like Transformer

- Custom transformer model (no positional embeddings)
- **Structure**: Embedding â†’ 2Ã— Transformer Blocks â†’ Global Avg Pooling â†’ Dropout â†’ Dense
- **Grid Search Parameters**:
  - `num_heads`: [2, 4]
  - `num_blocks`: [2, 4]
  - `feedforward_dim`: [128, 256]
- âœ… **Best Accuracy**:
  - **Validation**: 88.9%
  - **Test**: 88.26%

---

### ğŸ§¬ Autoencoder + Classifier

- Encoder: Embedding â†’ LSTM â†’ BatchNorm â†’ Repeat Vector â†’ LSTM â†’ Dense
- Classifier: Dense â†’ Dropout â†’ Dense â†’ Sigmoid
- **Grid Search Parameters**:
  - `embedding_size`: [256, 512]
  - `lstm_size`: [128, 256]
  - `dropout_rate`: [0.3, 0.5]
- âš ï¸ **Best Accuracy**:
  - **Validation**: 56.13%
  - **Test**: 53.2%

---

## ğŸ“ˆ Results Summary

| Model        | Test Accuracy |
|--------------|---------------|
| ğŸŒ€ CNN1D      | ğŸŸ¢ 97.89%      |
| ğŸ”  BERT       | ğŸŸ¡ 88.26%      |
| ğŸ§¬ Autoencoder| ğŸ”´ 53.2%       |

---

## ğŸ§  Observations

- **CNN1D** outperforms the rest â€” fast, accurate, and robust
- **BERT** performs well but shows slight **overfitting** past epoch 7
- **Autoencoder** struggles due to low latent diversity and complexity mismatch

---


